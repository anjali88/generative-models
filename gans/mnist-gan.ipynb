{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gan.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jhashekhar/generative-models/blob/master/gans/mnist-gan.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "wuyLrt1zOpvj",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "**Generative Adversarial Networks**\n",
        "\n",
        "New framework for estimating generative models via an adversarial process, in which we simutaneously train two models: a generative model G that captures the data distribution, and a discriminative model D that estimates the probability that a sample came from the training data rather than G.\n",
        "\n",
        "The training procedure for G is to maximize the probability of D making a mistake. "
      ]
    },
    {
      "metadata": {
        "id": "OpTFpnExQeuV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torchvision\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torchvision.utils import save_image\n",
        "from torchvision import transforms"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "kcjGG2wbJmKw",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "os.makedirs('images-gan1', exist_ok=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "lcKv6-GB6gXR",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "BATCH_SIZE = 64\n",
        "ngpu = 1\n",
        "dataset = torchvision.datasets.MNIST('./data', \n",
        "                                     download=True,\n",
        "                                     transform = transforms.Compose([\n",
        "                                         transforms.ToTensor(), \n",
        "                                         transforms.Normalize((0.5,), (0.5,))\n",
        "                                     ]))\n",
        "dataloader = torch.utils.data.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "M5GrF7wVKT5z",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if (torch.cuda.is_available() and ngpu > 0) else \"cpu\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "cFBDUtYDf-pP",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "def to_img(x):\n",
        "    x = 0.5 * (x + 1)\n",
        "    x = x.clamp(0, 1)\n",
        "    x = x.view(x.size(0), 1, 28, 28)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "TCMUcTOgRPrn",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Generator(nn.Module):\n",
        "  def __init__(self, ngpu):\n",
        "    super(Generator, self).__init__()\n",
        "    self.ngpu = ngpu\n",
        "    self.h1 = nn.Linear(64, 128)\n",
        "    self.h2 = nn.Linear(128, 512)\n",
        "    self.h3 = nn.Linear(512, 1024)\n",
        "    self.h4 = nn.Linear(1024, 28*28)\n",
        "    self.relu = nn.LeakyReLU(0.2)\n",
        "    self.tanh = nn.Tanh()\n",
        "    \n",
        "  def forward(self, x):\n",
        "    x = self.relu(self.h1(x))\n",
        "    x = self.relu(self.h2(x))\n",
        "    x = self.relu(self.h3(x))\n",
        "    x = self.tanh(self.h4(x))\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "GuJ05nHw1sUd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "class Discriminator(nn.Module):\n",
        "  def __init__(self, ngpu):\n",
        "    super(Discriminator, self).__init__()\n",
        "    self.ngpu = ngpu\n",
        "    self.h1 = nn.Linear(28*28, 512)\n",
        "    self.h2 = nn.Linear(512, 256)\n",
        "    self.h3 = nn.Linear(256, 1)\n",
        "    self.sigmoid = nn.Sigmoid()\n",
        "    self.relu = nn.LeakyReLU(0.2)\n",
        "    \n",
        "  def forward(self, x):\n",
        "    x = self.relu(self.h1(x))\n",
        "    x = self.relu(self.h2(x))\n",
        "    x = self.sigmoid(self.h3(x))\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vILPEakF4DWH",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "generator = Generator(ngpu).to(device)\n",
        "discriminator = Discriminator(ngpu).to(device)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "jVZYGjAmnv2w",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "criterion = torch.nn.BCELoss()\n",
        "optimizer_G = torch.optim.Adam(generator.parameters(), lr=0.0002, betas=(0.5, 0.999)) \n",
        "optimizer_D = torch.optim.Adam(discriminator.parameters(), lr=0.0002, betas=(0.5, 0.999))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "G5jawwlU49Tr",
        "colab_type": "code",
        "outputId": "19baa507-af74-429f-8819-aed69c75b2f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4012
        }
      },
      "cell_type": "code",
      "source": [
        "Tensor = torch.cuda.FloatTensor\n",
        "#-------------\n",
        "# Training\n",
        "#-------------\n",
        "num_epochs = 100\n",
        "latent_dim = 64\n",
        "sample_interval = 400\n",
        "samples = []\n",
        "sample_size = 24\n",
        "fixed_z = np.random.normal(0, 1, size=(sample_size, latent_dim))\n",
        "fixed_z = torch.from_numpy(fixed_z).float().to(device)\n",
        "\n",
        "# Train the network\n",
        "discriminator.train()\n",
        "generator.train()\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  \n",
        "  for i, data in enumerate(dataloader):\n",
        "    imgs, _ = data\n",
        " \n",
        "    # Ground truths\n",
        "    valid = Variable(Tensor(imgs.size(0), 1).fill_(1.0), requires_grad=False)\n",
        "    fake = Variable(Tensor(imgs.size(0), 1).fill_(0.0), requires_grad=False)\n",
        "    \n",
        "    real_imgs = Variable(imgs.type(Tensor))\n",
        "    real_imgs = real_imgs.view(-1, 784)\n",
        "   \n",
        "    #-----------------\n",
        "    # Train Generator\n",
        "    #-----------------\n",
        "    optimizer_G.zero_grad()\n",
        "    \n",
        "    # Sample noise as generator input\n",
        "    z = Variable(Tensor(np.random.normal(0, 1, (imgs.shape[0], latent_dim))))\n",
        "    \n",
        "    # Generate a batch of images\n",
        "    gen_imgs = generator(z)\n",
        "    \n",
        "    # Calculate Loss\n",
        "    g_loss = criterion(discriminator(gen_imgs), valid)\n",
        "    g_loss.backward()\n",
        "    optimizer_G.step()\n",
        "    \n",
        "    # ===================\n",
        "    # Train Discriminator\n",
        "    # ===================\n",
        "    optimizer_D.zero_grad()\n",
        "    \n",
        "    # Train with real image\n",
        "    real_loss = criterion(discriminator(real_imgs), valid)\n",
        "    \n",
        "    # Train with fake image\n",
        "    fake_loss = criterion(discriminator(gen_imgs.detach()), fake)\n",
        "    \n",
        "    d_loss = real_loss + fake_loss\n",
        "    d_loss.backward()\n",
        "    optimizer_D.step()\n",
        "    \n",
        "    batches_done = epoch * len(dataloader) + i\n",
        "    if batches_done % sample_interval == 0:\n",
        "      print(\"[Epoch %d/%d] [Batch %d/%d] [D loss: %f] [G loss: %f]\" % (epoch+1, num_epochs, i, len(dataloader), d_loss.item(), g_loss.item()))\n",
        "      \n",
        "   \n",
        "  generator.eval() # eval mode for generating samples\n",
        "  sample_z = generator(fixed_z)\n",
        "  samples.append(sample_z)\n",
        "  save_image(to_img(sample_z.data[:24]), 'images-gan1/%d.png' % batches_done, nrow=6, normalize=True)\n",
        "  generator.train()\n",
        "  "
      ],
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[Epoch 1/100] [Batch 0/938] [D loss: 0.673317] [G loss: 1.114533]\n",
            "[Epoch 1/100] [Batch 400/938] [D loss: 0.319829] [G loss: 1.850184]\n",
            "[Epoch 1/100] [Batch 800/938] [D loss: 0.425996] [G loss: 1.592158]\n",
            "[Epoch 2/100] [Batch 262/938] [D loss: 0.440708] [G loss: 2.058230]\n",
            "[Epoch 2/100] [Batch 662/938] [D loss: 0.328581] [G loss: 1.730249]\n",
            "[Epoch 3/100] [Batch 124/938] [D loss: 0.648256] [G loss: 1.075850]\n",
            "[Epoch 3/100] [Batch 524/938] [D loss: 0.376332] [G loss: 1.959702]\n",
            "[Epoch 3/100] [Batch 924/938] [D loss: 0.796608] [G loss: 1.449192]\n",
            "[Epoch 4/100] [Batch 386/938] [D loss: 0.453196] [G loss: 1.968834]\n",
            "[Epoch 4/100] [Batch 786/938] [D loss: 0.765518] [G loss: 1.779266]\n",
            "[Epoch 5/100] [Batch 248/938] [D loss: 1.035017] [G loss: 0.642479]\n",
            "[Epoch 5/100] [Batch 648/938] [D loss: 0.837807] [G loss: 1.837080]\n",
            "[Epoch 6/100] [Batch 110/938] [D loss: 0.627547] [G loss: 1.755863]\n",
            "[Epoch 6/100] [Batch 510/938] [D loss: 0.763461] [G loss: 2.841983]\n",
            "[Epoch 6/100] [Batch 910/938] [D loss: 1.083092] [G loss: 0.628159]\n",
            "[Epoch 7/100] [Batch 372/938] [D loss: 1.187979] [G loss: 2.385520]\n",
            "[Epoch 7/100] [Batch 772/938] [D loss: 0.930804] [G loss: 2.749228]\n",
            "[Epoch 8/100] [Batch 234/938] [D loss: 0.641403] [G loss: 1.530359]\n",
            "[Epoch 8/100] [Batch 634/938] [D loss: 0.755024] [G loss: 1.237864]\n",
            "[Epoch 9/100] [Batch 96/938] [D loss: 0.919519] [G loss: 0.866184]\n",
            "[Epoch 9/100] [Batch 496/938] [D loss: 1.126033] [G loss: 2.170125]\n",
            "[Epoch 9/100] [Batch 896/938] [D loss: 1.142122] [G loss: 1.264200]\n",
            "[Epoch 10/100] [Batch 358/938] [D loss: 1.004750] [G loss: 0.677315]\n",
            "[Epoch 10/100] [Batch 758/938] [D loss: 1.241503] [G loss: 2.238163]\n",
            "[Epoch 11/100] [Batch 220/938] [D loss: 0.989488] [G loss: 1.092316]\n",
            "[Epoch 11/100] [Batch 620/938] [D loss: 0.891265] [G loss: 1.200000]\n",
            "[Epoch 12/100] [Batch 82/938] [D loss: 1.027800] [G loss: 1.272944]\n",
            "[Epoch 12/100] [Batch 482/938] [D loss: 1.402307] [G loss: 0.450728]\n",
            "[Epoch 12/100] [Batch 882/938] [D loss: 1.334419] [G loss: 1.642592]\n",
            "[Epoch 13/100] [Batch 344/938] [D loss: 1.046398] [G loss: 1.160533]\n",
            "[Epoch 13/100] [Batch 744/938] [D loss: 1.428204] [G loss: 1.796314]\n",
            "[Epoch 14/100] [Batch 206/938] [D loss: 1.267774] [G loss: 1.779045]\n",
            "[Epoch 14/100] [Batch 606/938] [D loss: 1.175421] [G loss: 1.283941]\n",
            "[Epoch 15/100] [Batch 68/938] [D loss: 1.181551] [G loss: 0.647385]\n",
            "[Epoch 15/100] [Batch 468/938] [D loss: 1.072951] [G loss: 0.916234]\n",
            "[Epoch 15/100] [Batch 868/938] [D loss: 1.211548] [G loss: 0.720660]\n",
            "[Epoch 16/100] [Batch 330/938] [D loss: 1.139798] [G loss: 0.896187]\n",
            "[Epoch 16/100] [Batch 730/938] [D loss: 1.210988] [G loss: 0.638970]\n",
            "[Epoch 17/100] [Batch 192/938] [D loss: 1.008130] [G loss: 1.203617]\n",
            "[Epoch 17/100] [Batch 592/938] [D loss: 1.083378] [G loss: 1.164497]\n",
            "[Epoch 18/100] [Batch 54/938] [D loss: 1.254247] [G loss: 1.632036]\n",
            "[Epoch 18/100] [Batch 454/938] [D loss: 1.187461] [G loss: 0.666914]\n",
            "[Epoch 18/100] [Batch 854/938] [D loss: 1.038317] [G loss: 1.002944]\n",
            "[Epoch 19/100] [Batch 316/938] [D loss: 1.015026] [G loss: 1.320894]\n",
            "[Epoch 19/100] [Batch 716/938] [D loss: 1.172043] [G loss: 0.995460]\n",
            "[Epoch 20/100] [Batch 178/938] [D loss: 1.062071] [G loss: 1.026446]\n",
            "[Epoch 20/100] [Batch 578/938] [D loss: 1.019648] [G loss: 0.854765]\n",
            "[Epoch 21/100] [Batch 40/938] [D loss: 1.069383] [G loss: 0.810100]\n",
            "[Epoch 21/100] [Batch 440/938] [D loss: 1.090819] [G loss: 0.994668]\n",
            "[Epoch 21/100] [Batch 840/938] [D loss: 1.062564] [G loss: 1.241310]\n",
            "[Epoch 22/100] [Batch 302/938] [D loss: 1.127193] [G loss: 1.104357]\n",
            "[Epoch 22/100] [Batch 702/938] [D loss: 1.153002] [G loss: 0.926426]\n",
            "[Epoch 23/100] [Batch 164/938] [D loss: 1.109232] [G loss: 0.686277]\n",
            "[Epoch 23/100] [Batch 564/938] [D loss: 1.029520] [G loss: 1.025448]\n",
            "[Epoch 24/100] [Batch 26/938] [D loss: 1.170651] [G loss: 0.849606]\n",
            "[Epoch 24/100] [Batch 426/938] [D loss: 1.221869] [G loss: 0.808628]\n",
            "[Epoch 24/100] [Batch 826/938] [D loss: 1.315060] [G loss: 1.805724]\n",
            "[Epoch 25/100] [Batch 288/938] [D loss: 1.105798] [G loss: 0.760429]\n",
            "[Epoch 25/100] [Batch 688/938] [D loss: 1.209076] [G loss: 1.523300]\n",
            "[Epoch 26/100] [Batch 150/938] [D loss: 1.099833] [G loss: 1.354955]\n",
            "[Epoch 26/100] [Batch 550/938] [D loss: 1.289063] [G loss: 1.588360]\n",
            "[Epoch 27/100] [Batch 12/938] [D loss: 1.079337] [G loss: 1.336620]\n",
            "[Epoch 27/100] [Batch 412/938] [D loss: 1.139816] [G loss: 0.719498]\n",
            "[Epoch 27/100] [Batch 812/938] [D loss: 1.141996] [G loss: 1.327364]\n",
            "[Epoch 28/100] [Batch 274/938] [D loss: 1.053428] [G loss: 1.045252]\n",
            "[Epoch 28/100] [Batch 674/938] [D loss: 1.109881] [G loss: 0.952242]\n",
            "[Epoch 29/100] [Batch 136/938] [D loss: 1.127235] [G loss: 1.907918]\n",
            "[Epoch 29/100] [Batch 536/938] [D loss: 1.063272] [G loss: 1.267887]\n",
            "[Epoch 29/100] [Batch 936/938] [D loss: 1.036384] [G loss: 1.153998]\n",
            "[Epoch 30/100] [Batch 398/938] [D loss: 1.019230] [G loss: 1.029712]\n",
            "[Epoch 30/100] [Batch 798/938] [D loss: 1.015561] [G loss: 1.053802]\n",
            "[Epoch 31/100] [Batch 260/938] [D loss: 1.125942] [G loss: 0.901643]\n",
            "[Epoch 31/100] [Batch 660/938] [D loss: 1.329555] [G loss: 0.638050]\n",
            "[Epoch 32/100] [Batch 122/938] [D loss: 1.319820] [G loss: 0.639511]\n",
            "[Epoch 32/100] [Batch 522/938] [D loss: 1.235337] [G loss: 1.534625]\n",
            "[Epoch 32/100] [Batch 922/938] [D loss: 1.217870] [G loss: 0.716985]\n",
            "[Epoch 33/100] [Batch 384/938] [D loss: 1.219617] [G loss: 1.232852]\n",
            "[Epoch 33/100] [Batch 784/938] [D loss: 1.137152] [G loss: 0.802732]\n",
            "[Epoch 34/100] [Batch 246/938] [D loss: 1.081610] [G loss: 0.859716]\n",
            "[Epoch 34/100] [Batch 646/938] [D loss: 1.110252] [G loss: 1.468310]\n",
            "[Epoch 35/100] [Batch 108/938] [D loss: 1.277887] [G loss: 0.602154]\n",
            "[Epoch 35/100] [Batch 508/938] [D loss: 1.148631] [G loss: 1.506023]\n",
            "[Epoch 35/100] [Batch 908/938] [D loss: 1.158066] [G loss: 1.046137]\n",
            "[Epoch 36/100] [Batch 370/938] [D loss: 1.211275] [G loss: 1.546137]\n",
            "[Epoch 36/100] [Batch 770/938] [D loss: 1.016177] [G loss: 1.416805]\n",
            "[Epoch 37/100] [Batch 232/938] [D loss: 1.305936] [G loss: 1.829706]\n",
            "[Epoch 37/100] [Batch 632/938] [D loss: 1.073607] [G loss: 0.873438]\n",
            "[Epoch 38/100] [Batch 94/938] [D loss: 1.203236] [G loss: 1.347910]\n",
            "[Epoch 38/100] [Batch 494/938] [D loss: 1.161956] [G loss: 1.185087]\n",
            "[Epoch 38/100] [Batch 894/938] [D loss: 0.986854] [G loss: 1.110757]\n",
            "[Epoch 39/100] [Batch 356/938] [D loss: 1.066818] [G loss: 1.554138]\n",
            "[Epoch 39/100] [Batch 756/938] [D loss: 1.126674] [G loss: 1.380417]\n",
            "[Epoch 40/100] [Batch 218/938] [D loss: 1.009291] [G loss: 0.909475]\n",
            "[Epoch 40/100] [Batch 618/938] [D loss: 0.921346] [G loss: 1.098920]\n",
            "[Epoch 41/100] [Batch 80/938] [D loss: 1.107208] [G loss: 1.506384]\n",
            "[Epoch 41/100] [Batch 480/938] [D loss: 1.102161] [G loss: 1.536043]\n",
            "[Epoch 41/100] [Batch 880/938] [D loss: 1.136769] [G loss: 1.445972]\n",
            "[Epoch 42/100] [Batch 342/938] [D loss: 1.144233] [G loss: 0.794859]\n",
            "[Epoch 42/100] [Batch 742/938] [D loss: 1.150766] [G loss: 0.975724]\n",
            "[Epoch 43/100] [Batch 204/938] [D loss: 1.030096] [G loss: 0.784661]\n",
            "[Epoch 43/100] [Batch 604/938] [D loss: 1.046670] [G loss: 1.111980]\n",
            "[Epoch 44/100] [Batch 66/938] [D loss: 1.094285] [G loss: 1.009959]\n",
            "[Epoch 44/100] [Batch 466/938] [D loss: 1.222217] [G loss: 1.560833]\n",
            "[Epoch 44/100] [Batch 866/938] [D loss: 1.099046] [G loss: 1.144734]\n",
            "[Epoch 45/100] [Batch 328/938] [D loss: 1.119349] [G loss: 0.893661]\n",
            "[Epoch 45/100] [Batch 728/938] [D loss: 0.978473] [G loss: 1.104367]\n",
            "[Epoch 46/100] [Batch 190/938] [D loss: 1.122544] [G loss: 1.111596]\n",
            "[Epoch 46/100] [Batch 590/938] [D loss: 1.105223] [G loss: 0.912726]\n",
            "[Epoch 47/100] [Batch 52/938] [D loss: 1.104247] [G loss: 0.996644]\n",
            "[Epoch 47/100] [Batch 452/938] [D loss: 1.121724] [G loss: 1.448619]\n",
            "[Epoch 47/100] [Batch 852/938] [D loss: 1.146297] [G loss: 1.053263]\n",
            "[Epoch 48/100] [Batch 314/938] [D loss: 1.053601] [G loss: 1.007117]\n",
            "[Epoch 48/100] [Batch 714/938] [D loss: 1.158290] [G loss: 1.148195]\n",
            "[Epoch 49/100] [Batch 176/938] [D loss: 1.156524] [G loss: 0.783925]\n",
            "[Epoch 49/100] [Batch 576/938] [D loss: 1.198688] [G loss: 0.725412]\n",
            "[Epoch 50/100] [Batch 38/938] [D loss: 1.014838] [G loss: 1.199565]\n",
            "[Epoch 50/100] [Batch 438/938] [D loss: 1.013300] [G loss: 1.393116]\n",
            "[Epoch 50/100] [Batch 838/938] [D loss: 1.269203] [G loss: 1.736471]\n",
            "[Epoch 51/100] [Batch 300/938] [D loss: 1.141213] [G loss: 1.372316]\n",
            "[Epoch 51/100] [Batch 700/938] [D loss: 1.131166] [G loss: 1.238346]\n",
            "[Epoch 52/100] [Batch 162/938] [D loss: 1.176304] [G loss: 0.810190]\n",
            "[Epoch 52/100] [Batch 562/938] [D loss: 1.148566] [G loss: 0.842131]\n",
            "[Epoch 53/100] [Batch 24/938] [D loss: 1.040147] [G loss: 1.388469]\n",
            "[Epoch 53/100] [Batch 424/938] [D loss: 1.183887] [G loss: 1.171940]\n",
            "[Epoch 53/100] [Batch 824/938] [D loss: 1.099845] [G loss: 1.130020]\n",
            "[Epoch 54/100] [Batch 286/938] [D loss: 0.980038] [G loss: 1.472578]\n",
            "[Epoch 54/100] [Batch 686/938] [D loss: 0.991818] [G loss: 1.227343]\n",
            "[Epoch 55/100] [Batch 148/938] [D loss: 1.010306] [G loss: 1.205977]\n",
            "[Epoch 55/100] [Batch 548/938] [D loss: 1.111459] [G loss: 1.638281]\n",
            "[Epoch 56/100] [Batch 10/938] [D loss: 1.211570] [G loss: 0.676042]\n",
            "[Epoch 56/100] [Batch 410/938] [D loss: 1.014542] [G loss: 1.357004]\n",
            "[Epoch 56/100] [Batch 810/938] [D loss: 1.059990] [G loss: 0.915260]\n",
            "[Epoch 57/100] [Batch 272/938] [D loss: 0.956248] [G loss: 1.133647]\n",
            "[Epoch 57/100] [Batch 672/938] [D loss: 1.145091] [G loss: 1.237738]\n",
            "[Epoch 58/100] [Batch 134/938] [D loss: 1.103504] [G loss: 0.838184]\n",
            "[Epoch 58/100] [Batch 534/938] [D loss: 1.142653] [G loss: 1.029734]\n",
            "[Epoch 58/100] [Batch 934/938] [D loss: 1.073323] [G loss: 1.321720]\n",
            "[Epoch 59/100] [Batch 396/938] [D loss: 1.063924] [G loss: 1.010010]\n",
            "[Epoch 59/100] [Batch 796/938] [D loss: 1.034262] [G loss: 1.241835]\n",
            "[Epoch 60/100] [Batch 258/938] [D loss: 1.129114] [G loss: 1.523942]\n",
            "[Epoch 60/100] [Batch 658/938] [D loss: 1.260356] [G loss: 1.835422]\n",
            "[Epoch 61/100] [Batch 120/938] [D loss: 1.170094] [G loss: 0.724499]\n",
            "[Epoch 61/100] [Batch 520/938] [D loss: 1.010335] [G loss: 1.216094]\n",
            "[Epoch 61/100] [Batch 920/938] [D loss: 1.085671] [G loss: 1.136798]\n",
            "[Epoch 62/100] [Batch 382/938] [D loss: 0.988843] [G loss: 1.428207]\n",
            "[Epoch 62/100] [Batch 782/938] [D loss: 1.100221] [G loss: 1.548065]\n",
            "[Epoch 63/100] [Batch 244/938] [D loss: 0.924791] [G loss: 1.441194]\n",
            "[Epoch 63/100] [Batch 644/938] [D loss: 1.048318] [G loss: 1.099629]\n",
            "[Epoch 64/100] [Batch 106/938] [D loss: 1.116913] [G loss: 0.819465]\n",
            "[Epoch 64/100] [Batch 506/938] [D loss: 1.086869] [G loss: 1.237252]\n",
            "[Epoch 64/100] [Batch 906/938] [D loss: 1.080853] [G loss: 1.312044]\n",
            "[Epoch 65/100] [Batch 368/938] [D loss: 0.962531] [G loss: 1.327303]\n",
            "[Epoch 65/100] [Batch 768/938] [D loss: 0.912120] [G loss: 0.972349]\n",
            "[Epoch 66/100] [Batch 230/938] [D loss: 1.225264] [G loss: 0.864456]\n",
            "[Epoch 66/100] [Batch 630/938] [D loss: 1.022169] [G loss: 0.965804]\n",
            "[Epoch 67/100] [Batch 92/938] [D loss: 1.104551] [G loss: 0.881666]\n",
            "[Epoch 67/100] [Batch 492/938] [D loss: 1.097673] [G loss: 0.992814]\n",
            "[Epoch 67/100] [Batch 892/938] [D loss: 1.078435] [G loss: 1.325754]\n",
            "[Epoch 68/100] [Batch 354/938] [D loss: 1.067743] [G loss: 0.846789]\n",
            "[Epoch 68/100] [Batch 754/938] [D loss: 1.006732] [G loss: 1.021943]\n",
            "[Epoch 69/100] [Batch 216/938] [D loss: 1.059552] [G loss: 1.195330]\n",
            "[Epoch 69/100] [Batch 616/938] [D loss: 0.939333] [G loss: 1.137313]\n",
            "[Epoch 70/100] [Batch 78/938] [D loss: 1.061306] [G loss: 1.221617]\n",
            "[Epoch 70/100] [Batch 478/938] [D loss: 0.989421] [G loss: 1.135281]\n",
            "[Epoch 70/100] [Batch 878/938] [D loss: 1.093890] [G loss: 0.929601]\n",
            "[Epoch 71/100] [Batch 340/938] [D loss: 1.163135] [G loss: 1.477657]\n",
            "[Epoch 71/100] [Batch 740/938] [D loss: 1.042003] [G loss: 1.028085]\n",
            "[Epoch 72/100] [Batch 202/938] [D loss: 1.000953] [G loss: 1.096358]\n",
            "[Epoch 72/100] [Batch 602/938] [D loss: 1.143175] [G loss: 1.253670]\n",
            "[Epoch 73/100] [Batch 64/938] [D loss: 1.020894] [G loss: 1.276834]\n",
            "[Epoch 73/100] [Batch 464/938] [D loss: 1.007730] [G loss: 1.045750]\n",
            "[Epoch 73/100] [Batch 864/938] [D loss: 1.425510] [G loss: 0.520018]\n",
            "[Epoch 74/100] [Batch 326/938] [D loss: 1.032460] [G loss: 0.940629]\n",
            "[Epoch 74/100] [Batch 726/938] [D loss: 1.103136] [G loss: 1.154796]\n",
            "[Epoch 75/100] [Batch 188/938] [D loss: 1.131803] [G loss: 1.402957]\n",
            "[Epoch 75/100] [Batch 588/938] [D loss: 1.100522] [G loss: 1.035729]\n",
            "[Epoch 76/100] [Batch 50/938] [D loss: 1.264280] [G loss: 1.497420]\n",
            "[Epoch 76/100] [Batch 450/938] [D loss: 1.004246] [G loss: 1.351764]\n",
            "[Epoch 76/100] [Batch 850/938] [D loss: 1.058122] [G loss: 1.018855]\n",
            "[Epoch 77/100] [Batch 312/938] [D loss: 1.098807] [G loss: 1.268565]\n",
            "[Epoch 77/100] [Batch 712/938] [D loss: 1.026231] [G loss: 1.177021]\n",
            "[Epoch 78/100] [Batch 174/938] [D loss: 1.117480] [G loss: 0.920481]\n",
            "[Epoch 78/100] [Batch 574/938] [D loss: 1.174609] [G loss: 0.803425]\n",
            "[Epoch 79/100] [Batch 36/938] [D loss: 1.139204] [G loss: 1.092427]\n",
            "[Epoch 79/100] [Batch 436/938] [D loss: 0.987308] [G loss: 1.548192]\n",
            "[Epoch 79/100] [Batch 836/938] [D loss: 1.164018] [G loss: 1.217923]\n",
            "[Epoch 80/100] [Batch 298/938] [D loss: 1.084963] [G loss: 0.964985]\n",
            "[Epoch 80/100] [Batch 698/938] [D loss: 1.092965] [G loss: 1.087683]\n",
            "[Epoch 81/100] [Batch 160/938] [D loss: 1.170991] [G loss: 0.669650]\n",
            "[Epoch 81/100] [Batch 560/938] [D loss: 1.083794] [G loss: 1.577634]\n",
            "[Epoch 82/100] [Batch 22/938] [D loss: 0.999641] [G loss: 1.180649]\n",
            "[Epoch 82/100] [Batch 422/938] [D loss: 1.000604] [G loss: 1.020248]\n",
            "[Epoch 82/100] [Batch 822/938] [D loss: 1.121542] [G loss: 1.051568]\n",
            "[Epoch 83/100] [Batch 284/938] [D loss: 1.022965] [G loss: 1.067368]\n",
            "[Epoch 83/100] [Batch 684/938] [D loss: 0.989599] [G loss: 1.194856]\n",
            "[Epoch 84/100] [Batch 146/938] [D loss: 1.104838] [G loss: 1.125327]\n",
            "[Epoch 84/100] [Batch 546/938] [D loss: 1.108682] [G loss: 1.165158]\n",
            "[Epoch 85/100] [Batch 8/938] [D loss: 1.191578] [G loss: 1.254028]\n",
            "[Epoch 85/100] [Batch 408/938] [D loss: 1.055025] [G loss: 1.224704]\n",
            "[Epoch 85/100] [Batch 808/938] [D loss: 1.091266] [G loss: 0.869908]\n",
            "[Epoch 86/100] [Batch 270/938] [D loss: 1.147484] [G loss: 0.816107]\n",
            "[Epoch 86/100] [Batch 670/938] [D loss: 1.067031] [G loss: 1.160684]\n",
            "[Epoch 87/100] [Batch 132/938] [D loss: 1.189277] [G loss: 0.711546]\n",
            "[Epoch 87/100] [Batch 532/938] [D loss: 1.077399] [G loss: 1.581864]\n",
            "[Epoch 87/100] [Batch 932/938] [D loss: 1.107910] [G loss: 1.019189]\n",
            "[Epoch 88/100] [Batch 394/938] [D loss: 1.035500] [G loss: 1.243085]\n",
            "[Epoch 88/100] [Batch 794/938] [D loss: 1.145968] [G loss: 1.184298]\n",
            "[Epoch 89/100] [Batch 256/938] [D loss: 0.924472] [G loss: 0.981013]\n",
            "[Epoch 89/100] [Batch 656/938] [D loss: 1.026517] [G loss: 1.247538]\n",
            "[Epoch 90/100] [Batch 118/938] [D loss: 1.012336] [G loss: 1.828152]\n",
            "[Epoch 90/100] [Batch 518/938] [D loss: 1.029988] [G loss: 1.226397]\n",
            "[Epoch 90/100] [Batch 918/938] [D loss: 0.996291] [G loss: 1.245390]\n",
            "[Epoch 91/100] [Batch 380/938] [D loss: 0.948724] [G loss: 1.411715]\n",
            "[Epoch 91/100] [Batch 780/938] [D loss: 1.163459] [G loss: 0.885133]\n",
            "[Epoch 92/100] [Batch 242/938] [D loss: 0.954099] [G loss: 1.391233]\n",
            "[Epoch 92/100] [Batch 642/938] [D loss: 1.042603] [G loss: 1.389519]\n",
            "[Epoch 93/100] [Batch 104/938] [D loss: 1.083470] [G loss: 1.562275]\n",
            "[Epoch 93/100] [Batch 504/938] [D loss: 1.161935] [G loss: 0.887254]\n",
            "[Epoch 93/100] [Batch 904/938] [D loss: 1.139772] [G loss: 0.979031]\n",
            "[Epoch 94/100] [Batch 366/938] [D loss: 1.114155] [G loss: 0.936766]\n",
            "[Epoch 94/100] [Batch 766/938] [D loss: 1.227321] [G loss: 0.824789]\n",
            "[Epoch 95/100] [Batch 228/938] [D loss: 1.079667] [G loss: 1.004372]\n",
            "[Epoch 95/100] [Batch 628/938] [D loss: 1.031469] [G loss: 0.964815]\n",
            "[Epoch 96/100] [Batch 90/938] [D loss: 1.417011] [G loss: 2.097018]\n",
            "[Epoch 96/100] [Batch 490/938] [D loss: 1.193194] [G loss: 1.779377]\n",
            "[Epoch 96/100] [Batch 890/938] [D loss: 1.043245] [G loss: 1.294849]\n",
            "[Epoch 97/100] [Batch 352/938] [D loss: 1.053480] [G loss: 0.800087]\n",
            "[Epoch 97/100] [Batch 752/938] [D loss: 1.149783] [G loss: 0.718952]\n",
            "[Epoch 98/100] [Batch 214/938] [D loss: 1.090025] [G loss: 1.171933]\n",
            "[Epoch 98/100] [Batch 614/938] [D loss: 0.974178] [G loss: 1.433825]\n",
            "[Epoch 99/100] [Batch 76/938] [D loss: 0.890709] [G loss: 1.350040]\n",
            "[Epoch 99/100] [Batch 476/938] [D loss: 1.120105] [G loss: 1.681193]\n",
            "[Epoch 99/100] [Batch 876/938] [D loss: 1.013795] [G loss: 1.322939]\n",
            "[Epoch 100/100] [Batch 338/938] [D loss: 0.965627] [G loss: 1.339277]\n",
            "[Epoch 100/100] [Batch 738/938] [D loss: 0.992354] [G loss: 1.179892]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}